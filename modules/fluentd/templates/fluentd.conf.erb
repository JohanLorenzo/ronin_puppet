<source>
  @type exec
  command "log stream --color none --level debug --type log --style syslog \
      --timeout 1d \
    | tail -n +3 \
    | awk '{printf (/^[0-9]{4}/&&NR>1?RS:x)\"%s\",$0} END {print \"\"}'"
    # tail to omit first 2 lines that cause syslog parsing to fail:
    #   Filtering the log data using "type == 1024"
    #   Timestamp                       (process)[PID]    
    # awk remove newlines apple's log output as syslog prints in log messages
  <parse>
    @type syslog
    message_format auto
    time_type string
    time_format "%Y-%m-%d %H:%M:%S.%N%z"
    #time_format "%iso8601"
    utc true
    with_priority false
  </parse>
  tag system
  run_interval 60s  # run command if not still running
</source>

<match fluent.**>
  @type null
</match>

# Add a unique insertId to each log entry that doesn't already have it.
# This helps guarantee the order and prevent log duplication.
<filter system>
  @type add_insert_ids
</filter>

<filter system>
  @type record_transformer
  remove_keys ["host"]
  <record>
    hostname "#{Socket.gethostname}"
  </record>
  <record>
    workerId "#{Socket.gethostname.split('.')[0]}"
  </record>
  <record>
    workerType "<%= @worker_type %>"
  </record>
  <record>
    workerGroup "#{Socket.gethostname.split('.')[3]}"
  </record>
</filter>

<match system>
  @type copy
  #<store>
  #  @type stdout
  #</store>
  <store>
    @type google_cloud
    # Set the chunk limit conservatively to avoid exceeding the recommended
    # chunk size of 5MB per write request.
    buffer_chunk_limit 1M
    # Flush logs every 5 seconds, even if the buffer is not full.
    flush_interval 5s
    # Enforce some limit on the number of retries.
    disable_retry_limit false
    # After 3 retries, a given chunk will be discarded.
    retry_limit 3
    # Wait 10 seconds before the first retry. The wait interval will be doubled on
    # each following retry (20s, 40s...) until it hits the retry limit.
    retry_wait 10
    # Never wait longer than 5 minutes between retries. If the wait interval
    # reaches this limit, the exponentiation stops.
    # Given the default config, this limit should never be reached, but if
    # retry_limit and retry_wait are customized, this limit might take effect.
    max_retry_wait 300
    # Use multiple threads for processing.
    num_threads 8
    detect_json true
    # Use the gRPC transport.
    use_grpc true
    # If a request is a mix of valid log entries and invalid ones, ingest the
    # valid ones and drop the invalid ones instead of dropping everything.
    partial_success true
  </store>
</match>
